# Training Configuration for Recursive Code LLM

# Model Architecture
model:
  name: "RecursiveCodeLLM"
  vocab_size: 32000  # Will be set by tokenizer
  hidden_size: 768  # 512, 768, or 1024 depending on VRAM
  num_hidden_layers: 8  # 6-12 layers
  num_attention_heads: 12  # 8-16 heads
  intermediate_size: 3072  # Usually 4x hidden_size
  max_position_embeddings: 2048
  dropout: 0.1
  attention_dropout: 0.1
  use_flash_attention: false  # Set to true if flash-attn is installed
  layer_norm_eps: 1.0e-5
  initializer_range: 0.02

# Tokenizer
tokenizer:
  type: "BPE"
  vocab_size: 32000
  min_frequency: 2
  special_tokens:
    - "<|endoftext|>"
    - "<|pad|>"
    - "<|bos|>"
    - "<|eos|>"
    - "<|code|>"
    - "<|comment|>"
    - "<|docstring|>"

# Training
training:
  # Basic settings
  num_epochs: 10
  batch_size: 2  # Small for 8GB VRAM
  gradient_accumulation_steps: 16  # Effective batch size = 32
  learning_rate: 3.0e-4
  weight_decay: 0.01
  max_grad_norm: 1.0
  warmup_steps: 500

  # Mixed precision
  mixed_precision: "fp16"  # "no", "fp16", or "bf16"

  # Optimization
  optimizer: "adamw"
  lr_scheduler: "cosine"

  # Efficiency
  gradient_checkpointing: true

  # Seeds for reproducibility
  seed: 42

# Checkpointing
checkpoint:
  save_dir: "./checkpoints"
  save_steps: 500
  save_time_interval: 1800  # Save every 30 minutes (in seconds)
  keep_last_n: 3
  resume_from_checkpoint: true  # Auto-resume from latest

# Dataset
dataset:
  # Initial dataset
  initial_data_path: null  # Path to initial code dataset
  use_huggingface_dataset: true
  hf_dataset_name: "codeparrot/github-code"  # Or "bigcode/the-stack"
  hf_dataset_config: "python"
  max_length: 512

  # Dynamic dataset for recursive learning
  dynamic_dataset_path: "./data/generated_samples.jsonl"
  dynamic_dataset_version: 1
  max_dynamic_samples: 100000

  # Data processing
  num_workers: 4
  preprocessing_num_workers: 4

# Recursive Learning
recursive:
  enabled: true
  start_after_steps: 5000  # Start recursive loop after initial training
  generation_interval: 1000  # Generate new samples every N steps

  # Code generation
  generation:
    num_samples_per_iteration: 50
    max_length: 256
    temperature: 0.8
    top_p: 0.95
    top_k: 50
    num_return_sequences: 1

  # Evaluation
  evaluation:
    use_sandbox: true
    sandbox_timeout: 10  # seconds
    use_docker: false  # Set to true for better isolation
    docker_image: "python:3.10-slim"

    # Quality filters
    min_lines: 5
    max_lines: 100
    require_tests: false
    require_docstrings: false

  # Feedback loop
  feedback:
    success_threshold: 0.7  # Only add samples with >70% success
    max_retries: 3
    finetune_interval: 5000  # Finetune on new data every N steps
    finetune_epochs: 1

# Logging and Monitoring
logging:
  log_dir: "./logs"
  log_level: "INFO"
  log_to_file: true
  log_to_console: true

  # Wandb
  use_wandb: true
  wandb_project: "recursive-code-llm"
  wandb_entity: null  # Your wandb username
  wandb_run_name: null  # Auto-generated if null

  # TensorBoard
  use_tensorboard: true
  tensorboard_dir: "./logs/tensorboard"

  # Metrics to track
  log_interval: 10
  eval_interval: 500
  save_logs_steps: 100

# Hardware optimization
hardware:
  device: "cuda"  # "cuda" or "cpu"
  num_gpus: 1
  mixed_precision: true

  # Memory optimization
  gradient_checkpointing: true
  cpu_offload: false

  # DeepSpeed config (optional)
  use_deepspeed: false
  deepspeed_config: "./config/deepspeed_config.json"

# Evaluation
evaluation:
  eval_steps: 500
  eval_dataset_size: 1000
  metrics:
    - "perplexity"
    - "accuracy"
    - "code_quality"
    - "execution_success"
