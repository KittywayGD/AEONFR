# Recursive Code LLM

Un mod√®le de langage orient√© programmation qui s'entra√Æne de mani√®re r√©cursive et autonome sur GPU RTX 5060Ti.

## üéØ Objectif

Ce projet impl√©mente un syst√®me d'apprentissage autonome o√π un mod√®le de langage :
1. G√©n√®re du code Python
2. Ex√©cute et √©value le code g√©n√©r√©
3. Ajoute les solutions valides au dataset d'entra√Ænement
4. S'am√©liore continuellement en apprenant de ses propres g√©n√©rations r√©ussies

## üèóÔ∏è Architecture

### Mod√®le
- **Architecture** : Transformer decoder-only (style GPT)
- **Taille** : ~100M-500M param√®tres (optimis√© pour 8-16GB VRAM)
- **Couches** : 6-12 transformer blocks
- **Hidden size** : 512-1024
- **Attention heads** : 8-16
- **Optimisations** : Mixed precision (FP16/BF16), gradient checkpointing, gradient accumulation

### Composants Cl√©s

1. **Tokenizer Custom BPE** : Optimis√© pour le code Python
2. **Syst√®me de Checkpointing Robuste** : Sauvegarde automatique avec pause/reprise
3. **Boucle d'Auto-am√©lioration** : G√©n√©ration ‚Üí √âvaluation ‚Üí Validation ‚Üí Ajout au dataset
4. **Sandbox d'Ex√©cution** : Ex√©cution s√©curis√©e du code g√©n√©r√© (subprocess/Docker)
5. **Gestion de Dataset Dynamique** : Versioning et stockage des exemples valid√©s

## üìÅ Structure du Projet

```
coding-ai/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ training_config.yaml       # Configuration compl√®te
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architecture.py        # Mod√®le Transformer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenizer.py           # Tokenizer BPE custom
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py             # Boucle d'entra√Ænement
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ checkpoint.py          # Gestion des checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ recursive/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generator.py           # G√©n√©ration de code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py           # √âvaluation et ex√©cution
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feedback_loop.py       # Boucle r√©cursive
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îî‚îÄ‚îÄ dataset.py             # Gestion des datasets
‚îú‚îÄ‚îÄ checkpoints/                   # Sauvegardes du mod√®le
‚îú‚îÄ‚îÄ logs/                          # Logs d'entra√Ænement
‚îú‚îÄ‚îÄ tests/                         # Tests unitaires
‚îú‚îÄ‚îÄ train.py                       # Point d'entr√©e principal
‚îú‚îÄ‚îÄ requirements.txt               # D√©pendances
‚îî‚îÄ‚îÄ README.md                      # Ce fichier
```

## üöÄ Installation

### Pr√©requis
- Python 3.9+
- CUDA 11.8+ (pour GPU)
- 8-16GB VRAM GPU (RTX 5060Ti)
- ~50GB d'espace disque

### Installation des d√©pendances

```bash
cd coding-ai

# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# (Optionnel) Flash Attention pour meilleures performances
# pip install flash-attn --no-build-isolation
```

## ‚öôÔ∏è Configuration

Le fichier `config/training_config.yaml` contient tous les param√®tres configurables :

### Param√®tres Cl√©s √† Ajuster

```yaml
model:
  hidden_size: 768          # 512, 768, ou 1024 selon VRAM
  num_hidden_layers: 8      # 6-12 layers
  num_attention_heads: 12   # 8-16 heads

training:
  batch_size: 2             # Augmenter si VRAM > 8GB
  gradient_accumulation_steps: 16  # Effective batch = 32
  learning_rate: 3.0e-4
  num_epochs: 10

checkpoint:
  save_steps: 500           # Checkpoint tous les N steps
  save_time_interval: 1800  # Checkpoint tous les 30 min
  keep_last_n: 3            # Garder 3 derniers checkpoints

recursive:
  enabled: true             # Activer l'apprentissage r√©cursif
  start_after_steps: 5000   # Commencer apr√®s 5000 steps
  generation_interval: 1000 # G√©n√©rer tous les 1000 steps
```

## üèÉ Utilisation

### Entra√Ænement de Base

```bash
# D√©marrer l'entra√Ænement
python train.py --config config/training_config.yaml

# Reprendre depuis le dernier checkpoint
python train.py --config config/training_config.yaml --resume

# Utiliser un tokenizer pr√©-entra√Æn√©
python train.py --config config/training_config.yaml --tokenizer checkpoints/tokenizer.json
```

### Pause et Reprise

Le syst√®me g√®re automatiquement les interruptions :

```bash
# Pendant l'entra√Ænement, presser Ctrl+C pour une interruption propre
# Le syst√®me sauvegarde automatiquement l'√©tat complet

# Reprendre plus tard
python train.py --resume
```

### Monitoring

#### W&B (Weights & Biases)
```bash
# Activer dans config/training_config.yaml
logging:
  use_wandb: true
  wandb_project: "recursive-code-llm"
  wandb_entity: "votre-username"

# Se connecter √† W&B
wandb login
```

#### TensorBoard
```bash
# Lancer TensorBoard
tensorboard --logdir logs/tensorboard
```

## üìä M√©triques Suivies

### Entra√Ænement
- **Loss** : Cross-entropy loss
- **Perplexity** : Exp(loss)
- **Learning Rate** : Taux d'apprentissage courant
- **Epoch & Global Step** : Progression

### Apprentissage R√©cursif
- **Generation Rate** : Nombre de samples g√©n√©r√©s
- **Success Rate** : % de samples valides
- **Quality Score** : Score de qualit√© moyen
- **Dataset Size** : Taille du dataset dynamique
- **Execution Success** : % d'ex√©cutions r√©ussies

## üîß Optimisations pour RTX 5060Ti

Le projet est optimis√© pour fonctionner efficacement sur des GPU avec VRAM limit√©e :

1. **Mixed Precision (FP16)** : R√©duit l'utilisation m√©moire de ~50%
2. **Gradient Accumulation** : Simule des batch sizes plus grands
3. **Gradient Checkpointing** : √âconomise la m√©moire au co√ªt de ~20% de temps
4. **Small Batch Sizes** : batch_size=2 avec accumulation
5. **Efficient Attention** : Support optionnel de Flash Attention

### Estimation VRAM

| Config | Hidden Size | Layers | Parameters | VRAM (Train) | VRAM (Infer) |
|--------|-------------|--------|------------|--------------|--------------|
| Tiny   | 512         | 6      | ~100M      | ~6 GB        | ~2 GB        |
| Small  | 768         | 8      | ~200M      | ~8 GB        | ~3 GB        |
| Medium | 1024        | 12     | ~500M      | ~14 GB       | ~4 GB        |

## üîê S√©curit√© du Sandbox

Le code g√©n√©r√© est ex√©cut√© dans un environnement isol√© :

### Mode Subprocess (Par d√©faut)
- Ex√©cution dans un processus s√©par√©
- Timeout configurable (10s par d√©faut)
- Pas d'acc√®s r√©seau
- Ressources limit√©es

### Mode Docker (Recommand√© pour production)
```yaml
recursive:
  evaluation:
    use_docker: true
    docker_image: "python:3.10-slim"
```

Avantages :
- Isolation compl√®te
- Limites m√©moire/CPU strictes
- Pas d'acc√®s filesystem

## üìà Boucle d'Auto-am√©lioration

### Fonctionnement

1. **G√©n√©ration** : Le mod√®le g√©n√®re du code √† partir de prompts
2. **√âvaluation** :
   - V√©rification de syntaxe (AST parsing)
   - Ex√©cution dans sandbox
   - Calcul de m√©triques qualit√©
3. **Filtrage** : Seuls les samples avec score > seuil sont gard√©s
4. **Ajout au Dataset** : Les bons samples enrichissent le dataset
5. **Fine-tuning** : Entra√Ænement p√©riodique sur nouvelles donn√©es

### Prompts G√©n√©r√©s

Le syst√®me g√©n√®re automatiquement des prompts vari√©s :
- D√©finitions de fonctions
- Impl√©mentations de classes
- Algorithmes classiques
- Code avec docstrings
- R√©solution de probl√®mes

## üß™ Tests

```bash
# Ex√©cuter tous les tests
pytest tests/

# Tests sp√©cifiques
pytest tests/test_model.py
pytest tests/test_tokenizer.py
pytest tests/test_evaluator.py

# Avec couverture
pytest --cov=src tests/
```

## üêõ Debugging

### Logs

Les logs sont sauvegard√©s dans `logs/training.log` :

```bash
# Suivre les logs en temps r√©el
tail -f logs/training.log

# Rechercher des erreurs
grep ERROR logs/training.log
```

### Mode Debug

```yaml
logging:
  log_level: "DEBUG"  # Plus de d√©tails dans les logs
```

### Checkpoints Corrompus

```bash
# Lister les checkpoints disponibles
ls -lh checkpoints/

# Charger un checkpoint sp√©cifique
# Modifier checkpoint_manager.load_checkpoint() avec le chemin
```

## üöß Limitations Connues

1. **Python uniquement** : Pour l'instant, seul Python est support√©
2. **Tests limit√©s** : Pas de g√©n√©ration automatique de tests unitaires
3. **Qualit√© variable** : Les premi√®res it√©rations g√©n√®rent du code simple
4. **Compute intensif** : L'entra√Ænement complet peut prendre plusieurs jours

## üó∫Ô∏è Roadmap

- [ ] Support multi-langages (JavaScript, Go, Rust)
- [ ] G√©n√©ration automatique de tests
- [ ] √âvaluation bas√©e sur des benchmarks (HumanEval, MBPP)
- [ ] Fine-tuning avec RLHF
- [ ] Interface web pour monitoring
- [ ] Quantization (INT8/INT4) pour inference
- [ ] Support multi-GPU

## üìö R√©f√©rences

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer architecture
- [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - Decoder-only LM
- [CodeGen](https://arxiv.org/abs/2203.13474) - Code generation models
- [Self-Instruct](https://arxiv.org/abs/2212.10560) - Self-improvement approach

## ü§ù Contribution

Les contributions sont bienvenues ! N'h√©sitez pas √† :
- Ouvrir des issues pour bugs ou suggestions
- Proposer des PRs pour nouvelles fonctionnalit√©s
- Am√©liorer la documentation

## üìÑ Licence

MIT License - Voir LICENSE pour d√©tails

## üôè Remerciements

- HuggingFace pour les outils de tokenization et datasets
- PyTorch team pour le framework
- Accelerate pour l'entra√Ænement distribu√© optimis√©

---

**Note** : Ce projet est exp√©rimental et destin√© √† la recherche. N'utilisez pas le code g√©n√©r√© en production sans validation humaine approfondie.
